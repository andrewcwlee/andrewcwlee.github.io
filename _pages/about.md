---
permalink: /
#layout: archive
excerpt: "About me"
author_profile: true
redirect_from:
  - /about/
  - /about.html
---

<div hidden="hidden">
</div>

<span class="small_font">
  I am a Ph.D. student in Computer Science at the <a href="https://ucdavis.edu/" target="_blank">University of California Davis</a>, where I am part of the <a href="https://soltanilab.engineering.ucdavis.edu" target="_blank">Laboratory for AI, Robotics and Automation (LARA)</a>, under the guidance of Prof. <a href="https://faculty.engineering.ucdavis.edu/soltani/\" target="_blank">Iman Soltani</a>. Before starting my Ph.D., I was a Master‚Äôs student at the same lab, and I received my B.S. in Mechanical Engineering from <a href="https://www.hanyang.ac.kr/web/eng/home" target="_blank">Hanyang University</a> in Seoul, South Korea. <br><br>
  My research focuses on developing robotic systems capable of <b>mimicking "real" human behavior and dexterity</b> in complex and unstructured environments. In that regard, my recent works are aimed at bimanual coordination and human-like perception. 
</span>

<span class="small_font">
	Aside from research, I have a passion for photography, especially <a href="https://en.wikipedia.org/wiki/Street_photography" target="_blank">street photography</a>. It‚Äôs my way of reconnecting with the human side of things in this <b>üå™Ô∏è whirlwind üå™Ô∏è of technology</b>‚Äîcapturing moments that no algorithm could ever predict.
</span>

<div>
<div class="recent_updates">Updates</div>
<ul style="margin-top:-3px; background-color: #fafafa; padding-top: 20px; padding-bottom: 20px" class="updates">
	<li><span class="updates-month">OCT'24</span> <span class="updates-content"> We released the <a target='_blank' href='https://github.com/soltanilara/av-aloha'>code</a> and <a target='_blank' href='https://github.com/soltanilara/av-aloha-unity'>code (VR)</a> for AV-ALOHA!</span></li>
	<li><span class="updates-month">SEP'24</span> <span class="updates-content">"InterACT: Inter-dependency Aware Action Chunking with Hierarchical Attention Transformers for Bimanual Manipulation" got accepted at <b>CoRL 2024</b>!</span></li>
	<li><span class="updates-month">MAY'24</span> <span class="updates-content">I received the Summer Ph.D. Fellowship from UC Davis Computer Science Graduate Group!</span></li>
</ul>
</div>


<div class="recent_updates">Publications</div>
<span style="font-size:14px;margin-left: 25px;display: block;">
Most recent publications on <a style="text-decoration:none!important;" href="https://scholar.google.com/citations?user={{ author.googlescholar }}" target="_blank">Google Scholar</a>.<br>
<b>‚Ä°</b>indicates equal contribution.</span>

<div class="research-block">
	<div class="left">
		<span class="research-img">
			<img src="/images/teasers/avaloha-teaser.gif">
		</span>
	</div>
	<div class="right">
		<div class="title">Active Vision Might Be All You Need: Exploring Active Vision in Bimanual Robotic Manipulation</div>
		<div class="sub-title">Ian Chuang<sup>‚Ä°</sup>, <span class="author-me">Andrew Lee<sup>‚Ä°</sup></span>, Dechen Gao, Iman Soltani<br>
		<i>
		<b>
		<!-- Workshop on Whole-body Control and Bimanual Manipulation (WCBM) @ CoRL 2024<br> -->
		arXiv preprint 2024</b></i><br>
			<a target="_blank" class="tab_paper" href="https://soltanilara.github.io/av-aloha/">project page</a>
			<a target="_blank" class="tab_paper" href="https://arxiv.org/abs/2409.17435">arXiv</a>
			<a target="_blank" class="tab_paper" href="https://github.com/soltanilara/av-aloha">code</a>
			<a target="_blank" class="tab_paper" href="https://github.com/soltanilara/av-aloha-unity">code (VR)</a>
		</div>
		<span class="research-text">
		<b>tl;dr</b> We introduce AV-ALOHA, a bimanual robot system with 7-DoF active vision that is an extension of ALOHA 2. This system offers an immersive teleoperation experience using VR and serves as a platform to evaluate active vision in imitation learning and manipulation.
		</span>
	</div>
</div>

<div class="research-block">
	<div class="left">
		<span class="research-img">
			<img src="/images/teasers/interact-teaser.gif">
		</span>
	</div>
	<div class="right">
		<div class="title">InterACT: Inter-dependency Aware Action Chunking with Hierarchical Attention Transformers for Bimanual Manipulation</div>
		<div class="sub-title"><span class="author-me">Andrew Lee</span>, Ian Chuang, Ling-Yuan Chen, Iman Soltani<br>
			<i><b>CoRL 2024</b></i><br>
			<a target="_blank" class="tab_paper" href="https://soltanilara.github.io/interact/">project page</a>
			<a target="_blank" class="tab_paper" href="https://www.arxiv.org/abs/2409.07914">arXiv</a>
			<a target="_blank" class="tab_paper" href="https://openreview.net/forum?id=lKGRPJFPCM">OpenReview</a>
		</div>
		<span class="research-text">
		<b>tl;dr</b> InterACT builds on the concept of hierarchical attention mechanisms, capturing and extracting the inter-dependencies between dual-arm joint positions and visual inputs. By doing so, InterACT guides the two arms to perform bimanual tasks with precision‚Äîindependently yet in seamless coordination.
		</span>
	</div>
</div>
